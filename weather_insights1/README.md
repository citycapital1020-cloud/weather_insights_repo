# Weather Insights Project

## Overview
Weather Insights is a data engineering project designed to collect, process, and analyze weather data from multiple cities. The project leverages Apache Airflow for orchestration, Apache Spark for data processing, and PostgreSQL for data storage. It also integrates with Apache Superset for data visualization and reporting.

## Project Structure
- **config/**: Configuration files for the project.
- **dags/**: Airflow DAGs for orchestrating ETL workflows.
- **jars/**: JAR files required for Spark, such as PostgreSQL JDBC drivers.
- **logs/**: Log files generated by Airflow and Spark jobs.
- **new_weathersql/**: SQL schema definitions for the database.
- **notebooks/**: Jupyter notebooks for data exploration and transformation (raw, silver, gold layers).
- **plugins/**: Custom Airflow plugins.
- **sql_scripts/**: SQL scripts for schema creation and Superset initialization.
- **steps/**: Documentation and Power BI reports.
- **test_data/**: Sample CSV files for testing.
- **weather_data/**: Raw, deduped, and archived weather data files.
- **weather_notebooks/**: Additional notebooks and Spark warehouse files.
- **Dockerfile / Dockerfile.superset**: Docker images for Airflow and Superset services.
- **docker-compose.yaml**: Multi-container orchestration for the project stack.
- **requirements.txt**: Python dependencies.
- **superset_config.py**: Configuration for Apache Superset.

## Main Components
- **Airflow DAGs**: Automate the ETL pipeline from raw data ingestion to gold data output.
- **Spark Jobs**: Transform and deduplicate weather data (raw → silver → gold layers).
- **PostgreSQL**: Stores processed weather data for analytics and reporting.
- **Superset**: Provides dashboards and visualizations for weather insights.

## Getting Started
1. **Clone the repository**
2. **Build and start the stack**
   ```sh
   docker-compose up --build
   ```
3. **Access Services**
   - Airflow: http://localhost:8080
   - Superset: http://localhost:8088
   - PostgreSQL: localhost:5432 (user: pguser, password: pguser)
4. **Run Notebooks**
   - Use the notebooks in the `notebooks/` folder for data exploration and transformation.

## Data Pipeline
- **Raw Layer**: Ingests JSON weather data files.
- **Silver Layer**: Cleans, flattens, and deduplicates data using Spark.
- **Gold Layer**: Aggregates and prepares data for analytics and reporting.

## Customization
- Update Airflow DAGs in `dags/` to modify pipeline logic.
- Add new data sources or transformations in the Spark notebooks.
- Modify Superset dashboards as needed for new insights.

## Requirements
- Docker & Docker Compose
- Python 3.10+
- Apache Airflow
- Apache Spark
- PostgreSQL
- Apache Superset

## License
This project is for educational and demonstration purposes.
